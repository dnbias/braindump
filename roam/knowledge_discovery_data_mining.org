:PROPERTIES:
:ID:       fad85788-53f8-4de6-9e3c-775c3907e07c
:END:
#+title: Knowledge Discovery & Data Mining
#+date: [2023-02-27 Mon 10:53]
#+FILETAGS: erasmus university compsci

* Cycle of Analytics
** Preprocessing
- data transformation
- data reduction
  + in *big data* situation it is wise to choose a subset of the data to query.
  + reduce attributes or data
  + different strategies
    - *dimensionality reduction*
    - *numerosity reduction*
      + sampling
        - random selection, performance might suffer because of skew in the data
        - stratified sampling (cluster)
- cleaning of data
- integration of data
  + *assumption*: the data we have is incomplete
- data privacy concerns have to be considered

To accomplish this you can use visualizations, ask a domain expert.

*NB* there is no straightforward way to do all this, it takes intelligent thought.

The first goal is to arrive to a *clean and complete dataset*, one that fulfills the requirements.

** Interpretation
Statistics is not everything, the data experts/domain experts need to interpret the information the =ML= application produces.

From the novelties you extract from the data you take *action* based on them.

** Visualization
** Anonymity
See [[id:5e62675a-500a-41dc-a7ce-4fe6b3467c5a][k-Anonymity: A Model for Protecting Privacy]]

Privacy concerns are important to consider when studying datasets, personal information should not be exposed or even used.

We have to define security classes or accessibility control systems.
- a matrix $M(i,j)$ which specifies $\{Access, Update, Select\}$ for examples
  + where $i$ is an object (database views) and $j$ is a subject
- classes can be =Strictly Confidential= > =Confidential= > =Uncritical= and users are assigned one of these

Even sanitized data can still be /critical/ not for its contents per-se but for the possibility of being combined with other sources.

To solve this we can *generalize* the data. To do this we need the data to have some kind of hierarchy, either assumed or defined.

The approach is to use
a. $k$​-anonymity
   - generalisation and suppression of data values
   - find equivalence classes that contain $\geq k$ records
   - kinds of attributes
     + /uncritical data/ is not considered when constructing these classes
     + /non-sensitive data/ or /quasi-identifier/ are to be combined and generalized
     + /sensitive data/ has to be published
   - Samarati's approach[fn:s-approach Defined in: [[id:5a58e6f3-0dee-4b47-aa56-9b576f7a9e8e][k-Anonymous Data Mining: A Survey]]]:
     + different nodes indicate a combination of levels of generalization in each attribute
     + starting by the lowest node, the most specific, going up the data is generalized
     + /height/ is defined to the number of steps in the graph needed to traverse from the bottom to the top
     + start at level $$\lfloor\sqrt{\text{height}}\rfloor$$
b. $l$​-diversity[fn:diversity Defined in: [[id:a524447d-1d51-4c23-a5a4-ae2b617204d5][l-Diversity: Privacy Beyond k-Anonymity]]]
   - homogeneity in the sensitive attribute is not desirable as it offers some possible privacy exploits
   - $l$ number of minimum _different sensitive data values_ in an /equivalence class/
c. $t$​-closeness
   - $t$ measures the how similar the individual class distribution is against the total distribution, again the minimum value
   - higher is better again
   - *Earth-Mover distance* is one of the measures used

* Machine Learning
** Association Discovery
** Sequential Pattern Mining
** Classification
- supervised, training data accompanied by labels

2 steps
- model construction w/ training data
- model usage for future predictions
  + *accuracy is estimated*
    - /accuracy/ based on *test set*
    - a test set used to select models is called a *validation set*
*** Support Vector Machines
*** Decision Trees
** Clustering
- unsupervised
