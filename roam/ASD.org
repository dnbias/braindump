#+TITLE: Algoritmi e Strutture Dati
#+TEACHER: Andras Horvath(horvath@di.unito.it)
[[file:#university.org][#university]]
Esame
- scritto
- discussione laboratorio

Libro
- Introduzione agli algoritmi e strutture dati

* Problemi e Algoritmi
** Problemi Computazionali
Collezione di domande (istanze-ingressi) per cui sia stabilito un criterio astratto per riconoscere le risposte (uscite) corrette
- Es
  - Massimo comune divisore
  - moltiplicazione fra due interi
  - fattorizzazione
  - ordinamento
  - percorso ottimo in un grafo

Un Problema e' una _Relazione Binaria_
$R = {(istanza,risposta) : istanza, risposta soddisfano...}$
Il _dominio_ della relazione e' l'insieme delle domande possibili
R e' _univoca_ se ogni istanza ammette una sola risposta

** Algoritmi
Metodo meccanico per risolvere un Problema Computazionale
- Procedura
  + produce un output da qualsiasi input
- Algoritmo
  + procedura che termina per ogni ingressa ammissibile

Un algoritmo e' detto deterministico se sulla stesso input fornisce sempre lo stesso output
- ad ogni algoritmo deterministico possiamo associare una finzione input-output

Un algoritmo risolve R, ossia e' corretto rispetto ad R, se la sua funzione A associa una risposta ad ogni istanza di R

Un programma imlementa piu' algoritmi, inoltre implementa opportune strutture dati

*** Peak Finding
Input: vettore A[0...n-1] interi positivi
Output: un intero 0<= p <n t.c. A[p-1]<=A[p]>=A[p+1] dove A[-1]=A[n]=-inf

Peak(A)
for(i=0;i<length(A);i++)
    if(A[i]>=A[i-1] && A[i]>=A[i+1])
        return i
end for

**** Left Peak Finding
Nel caso migliore p=0 e' un picco
Nel caso peggiore p=n-1 confronti
**** Max Peak Finding
MAX-PEAK(A)
r = 0
i = 1
while(i<n-1)
    if A[i] > A[r]
        r <- i
    i <- i+1
end while
return r

Fa sempre lo stesso numero di confronti, sempre n-1

**** Analisi
Teorema picco
Si trovano segmenti sempre piu' corti su cui vale il teorema a partire da un q centrale.

**** Divide et Impera
PEAK-DI
p<-(i+j)/2
if A[p-1]<=A[p]>=A[p+1] then
    return p
else
    if A[p-1]>A[p] then
        return PEAK-DI(A,i,p-1)
    else
        return PEAK-DI(A,p+1,j)
    endif
endif

PEAK-FIND-DI(A,n)
return PEAK-DI(A,0,n-1)

- T(n)=
  - 1;        n=1
  - T(n/2)+1; n>1
$T(n)=T(\frac{n}{2^k})+k$ per $1\le k \le \log_{2}n$
$T(n)=1+\log_{2}n$
** Insolubilita' e Intrattabilita'
non tutti i problemi hanno una soluzione algoritmica (problema della terminazione)

intrattabile e' un problema che ha una soluzione algoritmica ma lo sforzo computazionale e' troppo grande

** Correttezza
Totale e Parziale

Specifica
- Precondizioni
- Postcondizioni
  - criterio che stabilisce come deve essere fatto l'output
**** Ricorsione
Un problema si presta alla ricorsione quando la ricorsione con un certo input ha una relazione semplice con la soluzione del problema con uno o piu' input diminuiti
#+begin_example
HANOI(n,solgente,destinazione,appoggio)
PRE:
- n>0
- base degli n dischi in alto ha diametro piu' piccolo del disco piu' in alto sia di destinazione che di appoggio
POST:
- torre di n dischi piu' in alto su sorgente e' spostata su destinazione

if n-1 then
    sposta un disco da sorgente a destinazione
else
    HANOI(n-1, sorgente, appoggio, destinazione)
    sposta un disco da solgente a destinazione
    HANOI(n-1, appoggio, destinazione, sorgente)
endif
#+end_example
#+begin_example
DIV-REC(a,b)
- Pre: a ge 0, b > 0
- Post: q, r t.c. a eq bq + r AND 0 le r < b
if a < b then
    q,r <- 0,a
else
    q',r <- DIV-REC(a-b,b)
    q <- q'+1
end if
return q,r
#+end_example
***** Schema dell'induzione semplice
1. Caso base P(1)
2. Passo induttivo P(m+1), P(m) e' l'ipotesi induttiva
3. 1. e 2. implicano che $\forall n \ge 1, P(n)$
***** Schema dell'induzione completa
1. Caso base $$
2. Passo induttivo
3. Conclusione
**** Iterazione
Divisione Intera
#+begin_example
DIV_IT(a,b)
while r ge b do
    r <- r-b
    q <- q+1
end while
return r,q
#+end_example

Si utilizzano le _invarianti_ per la dimostrazione di correttezza
- sempre vera:
  + inizializzazione
  + mantenimento
    - vale prima del ciclo --> vale anche dopo il corpo del ciclo
Va scelto in modo che sia utile per la dimostrazione di correttezza

** Terminazione
T2 temporal prover
- la non terminazione puo' essere semplicemente causata da un errore logico
- non terminazione implicita nel problema
  + problema $3n+1$
    - _Congettura di Collatz_
E' difficile dimostrare la terminazione se i parametri non decrescono in tutti i casi
Spesso gli algoritmi diminuiscono la dimensione dei parametri, l'ampiezza dell'intervallo

** Problema del Ordinamento - Sorting
*** Ricerca dicotomica o binaria
- dimezza la dimensione del problema ad ogni passo
#+begin_example
BinSearch-Ric(x,A,i,j)
- Pre: A[i...j] ordinato
- Post: true se x appartiene A[i...j]
if i>j then
    return false
else
    m <- floor((i+j)/2)
    if x eq A[m] then
        return true
    else
        if x<A[m] then
            return BinSearch-Ric(x,A,i,m-1)
        else
            return BinSearch-Ric(x,A,m+1,j)
        end if
    end if
end if
#+end_example
Casi
- best: $1$
- worst: $log_2 n$
*** Insertion Sort
per ordinare A[1...n]
- la parte A[1...i-1] gia' ordinato
- si puo' inserire l'elemento A[i] nella parte ordinata tramite scambi
  - se A[i] > A[i-1] -> A[1...i] e' ordinato e ci si ferma; altrimenti si scambia A[i] con A[i-1]
  - se A[i-1] > A[i-2] -> A[1...i] e' ordinato; altrimenti si scambia A[i-1] e A[i-2]
  - ...
Si parte inserendo A[2] poi si prosegue fino a n
#+begin_example
Insertion-Sort(A)
for i<-2 to length(A) do
    j<-i
    while j>1 and A[j-1]>A[j] do
        scambia A[j-1] con A[j]
        j<-j-1
    end while
end for
return A
#+end_example
**** Terminazione
assicurata dalla limitatezza dei cicli *for* e *while*
**** Correttezza
2 cicli -> 2 invarianti
1. A[1...i-1] e' ordinato
   a. corretto se il ciclo interno e' corretto
2. A[1...j-1] e A[j...i] sono ordinati AND A[1...j-1] le A[j+1...i]
All'uscita dell'algoritmo abbiamo i uguale a n+1 che implica che tutto il vettore A[1...n] e' ordinato
**** Complessita'
dipende da n e dalla distribuzione all'interno del vettore
assegnamo un costo ad ogni riga dell'algoritmo e lo moltiplichiamo alle volte per cui e' eseguito
1. for     - n volte
2. <-      - n-1
3. while   - sum_{i}^{n}{t_i-1}
   a. 1 nel caso migliore
   b. i nel caso peggiore
4. scambio - sum_{i}^{n}{t_i-1}

*Worst*
an^2 + bn + c
Nel caso peggiore Insert-Sort ha complessita' temporale quadratica
*Best*
dn + e
Nel caso miglione Insert-Sort ha complessita' temporale lineare
*** Selection Sort
Assumiamo che la parte sx del vettore sia gia' ordinata e che contenga elementi maggiori-uguale di questa parte a dx
- cerchiamo l'elemento minimo della parte dx e lo spostiamo in ultima posizione a sx (diminuendo la dimensione del problema)
#+begin_example
Selection-Sort(A)
for i <- 1 to length(A)-1 do
    j <- i+1
    k <- i
    while j < length(A)+1 do
        if A[j] < A[k] do
            k <- j
        end if
        j <- j+1
    end while
    scambia A[i] e A[k]
end for
return A
#+end_example
**** Terminazione
Implicata dalla terminazione dei cicli
**** Correttezza
2 Invarianti
1. A[1...i-1] ordinato e A[i...n] > uguale a A[1...i-1]
2. A[k] < uguale a A[i...j-1]

**** Complessita'
Sia nel caso migliore che nel caso poggiore,
Complessita' temporale quadratica

*** Alberi di Decisione
le foglie dell'albero devono essere tutte le possibili pormutazioni degli elementi del vettore
- $n!$
- per costruire un albero con un numero tale di foglie sono necessari almeno $log_2 n!$ livelli
- Usando la formula di Stirling per approssimare $n!$
  + $n log_2 n$
Che cresce molto piu' lentamente di una funzione quadratica
- cio' implica che esistano algoritmi molto piu' efficienti di quelli quadratici visti

** Complessitá di un algoritmo
Risorse utilizzate dall'algoritmo
- tempo
- spazio
- hardware
  + sempre piu' importante con piu' core e thread di esecuzione
Noi trattiamo la complessita' temporale
- per stimare la grandezza massima dell'ingresso(input) di un esecuzione ragionevole
- per confrontare l'efficienza di piu' algoritmi

Il tempo di calcolo e' una funzione rispetto all'input
Approcci, gli approcci differiscono solo di una costante moltiplicativa sotto certe condizioni:
- secondi di esecuzioni
- numero di operazioni elementari
- numero di volte una specifica operazione viene eseguita
  + piu' semplice

Una volta stabiliti i numeri di esecuzioni si passa all'analisi del caso migliore e del caso peggiore, si riconducono a polinomi

La *dimensione dell'ingresso* e' una misura della sua rappresentazione
- $|m| log_2 (m)+1$
- $|A[0...n-1]| nc$
  + $c$ numero bit del generico elemento di $A$
    - $c 1$ perche' le costanti moltiplicative non contano dal punto di vista dell'analisi asintotica

Fissato la dimensione esistano algoritmi per cui $T$ puo' cambiare rispetto alla forma dell'input
Distinguiamo i casi: migliore e peggiore
 - $T_{peggiore}(n) max{T()x}: |x|n$
 - $T_{migliore}(n) min{T()x}: |x|n$
Dobbiamo confrontare tra loro funzioni che hanno infiniti valori
- si trascura il numero finito di casi, conviene scegliere la funzione che cresce piu' lentamente all'infinito
  + se non ci interessano questi casi, se abbiamo informazioni in piu' allora vanno analizzati anche questi casi
Quanto contano le costanti?
- con un computer molto piu' veloce la dimensione massima trattabile cambia in maniera trascurabile
- la funzione che cresce meno velocemente e' comunque piu' importante di una costante moltiplicativa per il calcolo della complessita'
- inoltre la stima delle costanti e' molto difficile nella pratica

**** O-grande
Definito da P. Bachman, 1892.
$f(n) \in O(g(n)) \iff \exists c > 0, n_0 \forall n > n_0 \mid f(n) \le cg(n)$
Un $f(n)$ e' O-grande di $g(n)$ se e solo se $f(n)$ cresce al piu' come $g(n)$ dopo un numero finito di casi $n_0$ e eventuali costanti moltiplicative $c$.
Permette di specificare limiti superiori non stretti.
- $O(1)$
  + insieme delle funzioni superiormente limitate
    - la dimensione dell'input non ha impatto sul lavoro dell'algoritmo
Se $p(n)$ e' un polinomio di grado $k$ allora $p(k) in O(n^k)$

*Definizione equivalente*
$f(n) \in O(g(n)) \iff lim_{n \to \infty}\frac{f(n)}{g(n)}$ e
$0 \le lim_{n \to \infty} \frac{f(n)}{g(n)} < \infty$

*Teorema Utile*
$lim_{ n to infty}frac{f(n)}{g(n)} eq 0 <-> f(n) in O(g(n)) and g(n) notin O(f(n))$

- *NB*
  + nei polinomi cio' che conta e' il termine di grado piu' alto: il grado del polinomio
  + nei logaritmi non conta la base per O-grande
    + $O(\log_a n) = O(\log_b n), a,b >1$

- *Inclusioni*
  - $O(1) \subset O(\log n)$

  - $O(\log n) \subset O(n)$

  - $O(n) \subset O(n \log n)$

  - $O(n^p) \subset O(2^n)$

  - $O(2^n) \subset O(3^n)$
Il tempo di calcolo sufficiente alla risoluzione del problema é il suo confine superiore
- confine superiore alla complessitá di un problema
Il tempo di calcolo necessario alla risoluzione del problema
- confine inferiore alla complessitá del problema, per i tempi di calcolo di tutti gli algoritmi che risolvono il problema
- banali
  + dimensione del input
  + dimensione del output
  + eventi contabili
**** Omega
$\Omega$ limite asintotico inferiore
$f(n) \in \Omega(g(n)) \iff \exists c > 0, n_0 \forall n > n_0 \mid cg(n) \le f(n)$
$0< lim_{n \to \infty} \frac{f(n)}{g(n)} \le \infty$

**** Teta
\Theta limite asintotico sia inferiore sia superiore
$f(n) \in \Theta(g(n)) \iff \exists c_1 > 0,c_2 >0, n_0 \forall n > n_0 \mid c_1 g(n) \le f(n) \le c_2 g(n)$
$f(n) \in \Theta(g(n)) \iff f(n) \in O(g(n)) \land f(n) \in \Omega (g(n))$
$0< lim_{n \to \infty} \frac{f(n)}{g(n)} < \infty$
**** o-piccolo
$f(n) \in (g(n)) \iff \forall c > 0 \exists n_0 \forall n > n_0 \mid f(n) \le cg(n)$
$f(n)$ é un infinitesimo di $g(n)$

*** Somma-17

#+begin_example
Somma-17(V)
- Pre: V é un vettore che contiene numeri positivi
- Post: True se ci sono due numeri a,b t.c. a+b=17, False altrimenti

boolean b = False
for i=0 to length(V)-1
  for j=i+1 to length(V)-1
    if V[] + V[] == 17
      b = True
    end-if
  end-for
end-for
return b
#+end_example

$O(n^2)$ per il numero di volte che viene eseguito l'if nel caso peggiore
$\Omega(n)$ per la dimensione dei dati


_Soluzione di complessita' lineare_
#+begin_example
Somma-17-Lineare(V)

bool C[18]
int i, j
for i=0 to 17
  C[i] = False
end-for
for i=0 to length(V)-1
  if V[i] <= 17
    C[V[i]] = True
end-for
for i=0,j=17; i<j && !(C[i] && C[j]); i++,j--
end-for
return i < j
#+end_example
$T_{Somma-17-Lineare}(n) \in O(18+n+9+1) \in O(n)$
Questo implica che l'algoritmo é ottimo in quanto $\Omega(n)$ é confine inferiore del problema.

*** Minimo
$cn + d le T(n) le an+b$
- contenuto tra funzioni lineari
  + al piu' lineare
** Relazioni di Ricorrenza
La funzione tempo di un algoritmo ricorsivo é a sua volta ricorsiva: é detta *Relazione di ricorrenza*
- Calcolo ricorsivo del fattoriale

$T(n) = c$ se $n=0$
$T(n) = T(n-1)+d$ altrimenti

#+begin_src C
int min_ric(int a[], int i){
  if (i = length(a))
      return a[i];
  else
      return min(a[i], min_ric(a, i+1));
}
#+end_src

Nel caso dell'algoritmo della torre di Hanoi ci si riconduce ad una sommatoria di progressione geometrica.

$T(n)=c^n b + \frac{c^n - 1}{c - 1}d$
$T(n) \subset \Theta(c^n)$
*** Quick Sort
Sceglie un perno e riorganizza il vettore per avere elementi minori di $q$ prima di questo maggiori dopo.
Ogni passo se necessario vá partizionato il vettore.
#+begin_src C
  int partition(int a[],int s, int n){
      int i=s;
      int j = n-1;
      while(i<=j){
          if(a[i] <= a[0]){
              i += 1;
          }
          else if(a[j] > a[1]){
              j -= 1;
          }
          else {
              int temp = a[i];
              a[i] = a[j];
              a[j] = temp;
          }
      }
      int temp = a[j];
      a[j] = a[s];
      a[s] = temp;
      return j;
  }

  void quick_sort(int a[]){
      int n = length(a)-1;
      if(n>0){
          int p = partition(a,0,n);
          if(p>2)
              quick_sort(a,0,n);
          if(p<n-1)
              quick_sort(a,p+1,n);
      }
  }
#+end_src

Per dimostrarne la correttezza va utilizzata l'induzione completa, non semplice.
- in quanto la dimensione delle due chiamate ricorsive operano su dimensioni ignote minori di n elementi
Uno volta dimostrata la correttezza di =partizione= la dimostrazione é banale
**** Complessitá
Il partizionamento esamina una volta ogni elemento: é lineare
$T_p(n)=an$
Identifichiamo le situazioni estreme della ricorsione di quicksort
1. due partizioni con lo stesso numero degli elementi
2. una contiene tutti gli elementi e una é vuota

3. da luogo ad una relazione di  ricorrenza
   - $T(n) = c$ con $n=1$

   - $T(n)= T(n-1)+T_p(n)+b$ altriment

*** Relazioni Lineari a Partizione costante
Teorema master per relazioni lineari in termini $O()$, con $a$ che rappresenta il numero di chiamate ricorsive.
h: quanto diminuisce la dimensione del problema
a: numero delle chiamate ricorsive
b e c: quanto tempo impiegano le parti non ricorsive
- $a=1$: $T(n)\subset O(n^{b+1})$
- $a\ge 2$: $T(n)\subset O(a^n n^b)$

Il risultato dá meno informazioni rispetto alla sostituzione o l'iterazione, che dá informazioni rispetto a $\Theta()$.
Puó anche non fornire il limite piú stretto possibile.
** Divide et Impera - Relazioni lineari a partizione bilanciata
~Teorema~
$T(n) = d$ se $n=1$
$T(n) = aT(n/b)+cn^\beta$ se $n=1$
allora:
posto $\alpha = \log a / \log b$
$\alpha > \beta$: $T(n) \subset O(n^\alpha)$
$\alpha = \beta$: $T(n) \subset O(n^\alpha \log n)$
$\alpha < \beta$: $T(n) \subset O(n^\beta)$ 
*** Minimo e Massimo
*** Merge Sort
Fondere array ordinati impiega molto meno tempo rispetto che altrimenti
#+begin_src C
  int *merge(int b[],int c[]){
      if (b == NULL)
          return c;
      else if(c == NULL)
          return B;
      else if(b[1] <= c[1])
          return {b[1],merge(b[2...length(b)],c)};
      else
          return {c[1],merge(b,c[2...length(b)])};
  }


  int *merge_sort(int a[], int i, int n){
      if(n-i == 1)
          return a;
      else{
          int k = (n-i)/2;
          int b[] = merge_sort(a, 1, k);
          int c[] = merge_sort(a, k+1, n);
          return merge(b,c);
      }
  }
+end_src

**** Complessita'
$T(n)=2T(n/2)+n$
$T(n)=\log_2 n \cdot n . cn \subset \Theta(n \log n)$
L'algoritmo e' ottimo.

*** Quick Sort: caso medio
Il caso peggiore e' noto (quadratico), il caso migliore sara' come il Merge Sort (n log n)

Si dimostra che e' ottimo con $O(n \log n)$
** Programmazione Dinamica
Si basano come i Divide et Impera sulla scomposizione ricorsiva di un problema in sottoproblemi per poi ricomporli
- *DI* efficiente se i sottopreblemi sono indipendenti, altrimenti puo' fare lo stesso lavoro piu' di una volta
- *DI* puo' essere molto inefficiente se i sottoproblemi non sono indipendenti tra loro
Al contrario *PD* puo' semplificare molto il problema.

Il problema deve possedere due proprieta'
1. =sottostruttura della soluzione=
   - la soluzione del sottoproblema e' un sottoinsieme del problema
2. =sottoproblemi ripetuti=
   - una soluzione deve essere riutilizzabile in un altro sottoproblema
   - annotazione dei risultati piu' semplici
     - [[file:20210402203855-memoization.org][Memoization]]
   - per efficienza di memoria si sviluppa un approccio [[file:20210402203940-bottom_up.org][Bottom-up]]

Prima si sviluppa una soluzione iterativa, poi la si migliora con le tecniche della *PD*
*** Successione di Fibonacci
$f_0 = 0$, $f_1=1$
$f_n = f_{n-2} + f_{n-1}$ per $n>1$
#+begin_example
Fib(n)
if n <= 2 then
  f = 1
else
  f = Fib(n-1) + Fib(n-2)
endif
return f
#+end_example
La relazione di ricorrenza del numero di nodi $N_n$ nell'albero delle chiamate e' simile a quella della sequenza di Fibonacci $f_n$. Cambia per un +1.
La _formula di Binet_ permette il calcolo del ennesimo Fibonacci.

$N_n \subset \Omega(\phi^n)$
Quindi ha crescita esponenziale, almeno.

Molto lento  perche' e' richiesto il calcolo della stessa cosa ripetutamente
- percio' implementiamo la memoization: approccio [[file:20210402210717-top_down.org][Top-down]]
  - lo spazio utilizzato per migliorare l'algoritmo cosi' e' $\Theta(n)$
L'albero ha uno sviluppa lineare verso sinitra, anche il tempo sara $\Theta(n)$

#+begin_example
Fib-BottomUp(n)
if n <= 2 then
  return 1
else
  Fib[1] = 1, Fib[2] = 1
  for i=3 to n do
    Fib[i] = Fib[i-1]+Fib[i-2]
  end-for
endif
return Fib[n]
#+end_example
Tempo e spazio sono $\Theta(n)$

L'array puo' essere eliminato, servono solo gli ultimi due numeri
#+begin_example
Fib-Iter(n)
if n <= 2 then
  return 1
else
  FibA = 1, FibB = 1
  for i=3 to n do
    tmp = FibA+FibB
    FibB = FibA
    FibA=tmp
  end-for
endif
return FibA
#+end_example
Il tempo di calcolo e' sempre $\Theta(n)$
ma lo spazio  ora e' $\Theta(1)$
*** Massima Sottosequenza Comune - LCS
=Longest Common Subsequence=
Trattando Stringhe di DNA
Non si intendono elementi necessariamente successivi
- CS == sotto-sequenza
Z é LCS se
- Z cs X && Z cs Y && Z ha lunghezza massima

*Proprietá della sottostruttura*
- mettere in relazione $LCS(X,Y)$ con la soluzioni che coinvolgono prefissi di $X$ e $Y$
- definiamo $k,m,n$ come lunghezze di $Z,X.Y$
- due casi
  + $x_m = y_n$
    - se $Z$ é $LCS(X,Y)$ allora $z_k$ sará $x_m$
      + allora $Z_{k-1}$ é $LCS(X_{m-1},Y_{n-1})$
  + $x_m \neq y_n$
    - se $Z$ é $LCS(X,Y)$ allora $z_k$ puó essere $x_m$ o $y_n$ o qualsiasi altro
      + sicuramente $x_m$ o $y_n$ non serve per formare $Z$, anche entrambi
      + allora $Z_{k-1}$ é $LCS(X_{m-1},Y_{n})$ o $LCS(X_{m},Y_{n-1})$ o entrambi
=Teorema=
- se $i=0 || j=0$
  + < >
- se $x_i = y_j$
  + $LCS(X_{i-1}, Y_{j-1}) + x_i$ dove $+$ indica la concatenazione
- se $x_i \neq y_j$
  - longest($LCS(X_{i-1},Y)$, $LCS(X,Y_{j-1})$)

Il Tempo di calcolo di questa procedura ricorsiva é esponenziale:
$T(k)=2T(k-1)+1 \in \Theta(2^k) =  \Theta(2^{m+n})$

Si costruiscono due tabelle basate sullo schema ricorsivo
$c[0..m, 0..n]$ e $b[1..m, 1..n]$

**** Algoritmo Bottom-up
permette di ottimizzare riempiendo una tabella $m \cdot n$
$T(k) \in \Theta(m \cdot n)$


* Strutture Dati
** Insiemi Dinamici
Possibili in array, liste o hash, ma con tempo di calcolo diversi
- elementi finiti
- gli elementi possono cambiare
- il loro numero puó cambiare
- si assume che ogni elemento abbia un attributo chiave
- si assume che le chiavi sono tutte distinte
** Array
*** Statici
*** Ridimensionabili
** Liste
** Hashing
*** Tavole a indirizzamente diretto
*** Tavole di hash
*** Tavole hash con concatenamento
*** Funzioni hash
*** Indirizzamente aperto

* Laboratorio
** Fonti utili
- Cormer, Leiserson, Rivest, Stein: Introduzione agli algoritmi e strutture dati
- Pro Git
  + bibbia di git
- Junit 4
  + per lo unit testing in java
** Indicazioni per il progetto
Vedere [[https://gitlab2.educ.di.unito.it/pozzato/laboratorio-algoritmi-2020-2021][GitLab]] del dipartimento.
- Negli esercizi dove é richiesta l'implementazione di una libreria + applicazione
  + netta separazione fra libreria e applicazione
  + lib deve offrire un insieme di funzionalitá potenzialmente utili a qualunque utili a qualunque applicazione
  + l'implementazione nen deve essere influenzato in alcun modo dagli usi di essa eventualmente richiesti
- Nelle librerie
  + information hiding
    - non definire metodi pubblici se hanno solo uso interno
    - Java: utilizzo adeguato dei modificatori di accesso
    - C: suddividere le dichiarazioni e l'implementazione fra header e file .c
  + i metodi esposti non devono richiedere la conoscenza dell'implementazione specifica
- Modularizzare il codice
  + lunghezza: 30 righe considerando anche i commenti e whitespace
  + commenti prima di una definizione che spieghi il funzionamento dell'oggetto definito
    - evitare il commentare direttamente il codice in se, dovrebbe essere chiaro
- Nomi *significativi* e in *inglese*
  + Java: package, TheClass, theMethod(), THE_CONSTANT
  + C (convenzioni GTK+): THE_MACRO, THE_CONSTANT, TheType, TheStruct, the_function()
** Unit Testing
- *UNIT Test*
  + verificano porzioni di codice
  + test funzionali, verificano correttezza
    - piccoli e autocontenuti
    - test di singole unitá di codice
  + predipongono un input
  + invocano la unit
  + verificano che output o side-effect sia corretto
    - attraverso asserzioni
      + asserzione del risultato atteso
      + in caso di fallimento il test viene interrotto ed é restituito un messaggio d'errore
  + un classico approccio é quello di scrivere tutti gli unit test prima di implementare il codice
  + test su casi limiti e casi semplici
  + devono essere *focalizzati*
    - un singolo unit test deve focalizzarsi su un solo unit
    - se sono presenti piú asserzioni questo puó essere un segnale di poca focalizzazione del test
  + devono essere *indipendenti*
    - l'ordine non deve mai influire sul loro risultato
    - input e output di test diversi non devono essere comunicanti tra loro
    - questo vincolo é imposto da Junit ricaricando l'intera classe in memoria prima di eseguire ciascun metodo di test e seguendo i test in ordine casuale
  + devono essere *automatici*
    - non devono richiedere l'intervento umano
    - non devono dare output interni
- Una suite di unit test migliora la documentazione di un progetto

Java: =JUnit=
C:    =Unity=

** Git
Sistema di Versioning del software
- [[https://git-scm.com/book/en/v2][Pro Git]]
-

** Java
*** Documentazione secondo JavaDoc
*** Hash-code
Fare attenzione che sia consistente con Equals
- quando si implementa =compare=
*** BufferedReader
Utilizzato per leggere ~file.csv~ di input
*** Try with resources
*** String.split()
** C
*** Documentare nell'header
*** Generici non disponibili
Utilizziamo puntatori a puntatori void
- ~void** array~
*** Comparatori
Si richiede all'utente un puntatore ad una funzione
che implementa il comparator
*** Unit Testing
- Assert
- Unity
  + non é uno standard ma é utile

** Algoritmi
*** [[file:20210409200632-generic_merge_binary_insertion_sort.org][Generic Merge-Binary Insertion Sort]]
*** [[file:20210409200833-dynamic_edit_distance.org][Dynamic Edit Distance]]
